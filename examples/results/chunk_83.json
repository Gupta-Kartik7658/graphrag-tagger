{
  "chunk": "Graph Retrieval-Augmented Generation: A Survey 111:23 8\nTraining\nIn this section, we summarize the individual training of retrievers, generators, and their joint training. We categorize previous works into Training-Free and Training-Based approaches based on whether explicit training is required. Training-Free methods are commonly employed when using closed-source LLMs such as GPT-4 [127] as retrievers or generators. These methods primarily rely on carefully crafted prompts to control the retrieval and generation capabilities of LLMs.\nDespite LLMsâ€™ strong abilities in text comprehension and reasoning, a challenge of Training-Free methods lies in the potential sub-optimality of results due to the lack of specific optimization for downstream tasks. Conversely, Training-Based methods involve training or fine-tuning models using supervised signals. These approaches enhance the model performance by adapting them to specific task objectives, thereby potentially improving the quality and relevance of retrieved or generated content. Joint training of retrievers and generators aims to enhance their synergy, thereby boosting performance on downstream tasks. This collaborative approach leverages the complementary strengths of both components to achieve more robust and effective results in information retrieval and content generation applications.",
  "source_file": "Sample PDf\\Graph Retrieval-Augmented Generation.pdf",
  "classification": {
    "content_type": "paragraph",
    "is_sufficient": true,
    "topics": [
      "Natural Language Processing",
      "Retrieval Methods",
      "Training Techniques"
    ]
  }
}