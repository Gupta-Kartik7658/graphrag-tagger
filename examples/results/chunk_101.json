{
  "chunk": "111:28\nPeng et al.\nLLMs with graphs, which contains 1,740 questions that can be answered with the knowledge from 10 domain graphs. CRAG [186] provides a structured query dataset, with additional mock APIs to access information from underlying mock KGs to achieve fair comparison.\n9.3.2\nMetrics. The evaluation metrics for GraphRAG can be broadly categorized into two main types: downstream task evaluation (generation quality) and retrieval quality.\n(1) Downstream Task Evaluation (Generation Quality). In the majority of research studies, downstream task evaluation metrics serve as the primary method for assessing GraphRAGâ€™s performance.\nFor example, in KBQA, Exact Match (EM) and F1 score are commonly used to measure the accuracy of answering entities. In addition, many researchers utilize BERT4Score and GPT4Score to mitigate instances where LLMs generate entities that are synonymous with the ground truth but not exact matches. In CSQA, Accuracy is the most commonly used evaluation metric. For generative tasks such as QA systems, metrics like BLEU, ROUGE-L, METEOR, and others are commonly employed",
  "source_file": "Sample PDf\\Graph Retrieval-Augmented Generation.pdf",
  "classification": {
    "content_type": "paragraph",
    "is_sufficient": true,
    "topics": [
      "Topic 4",
      "Topic 10"
    ]
  }
}